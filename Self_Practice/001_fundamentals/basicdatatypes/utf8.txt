UTF-8 is the most commonly used encoding for Unicode. Unicode uses 4
bytes (32-bits) to represent each code point, the technical name for each
character and modifier. Given this, the simplest way to represent Unicode
code points is to store 4 bytes for each code point. This is called UTF-32. It
is mostly unused because it wastes so much space. Due to Unicode
implementation details, 11 of the 32 bits are always zero. Another common
encoding is UTF-16, which uses one or two 16-bit (two byte) sequences to
represent each code point. This is also wasteful; much of the content in the
world is written using code points that fit into a single byte. And that’s where
UTF-8 comes in.
UTF-8 is very clever. It lets you use a single byte to represent the Unicode
characters whose values are below 128 (which includes all of the letters,
numbers, and punctuation commonly used in English), but expands to a
maximum of 4 bytes to represent Unicode code points with larger values.
The result is that the worst case for UTF-8 is the same as using UTF-32.
UTF-8 has some other nice properties. Unlike UTF-32 and UTF-16, you
don’t have to worry about little-endian vs. big-endian. It also allows you to
look at any byte in a sequence and tell if you are at the start of a UTF-8
sequence, or somewhere in the middle. That means you can’t accidentally
read a character incorrectly.
The only downside is that you cannot randomly access a string encoded with
UTF-8, While you can detect if you are in the middle of a character, you
can’t tell how many characters in you are. You need to start at the beginning
of the string and count. Go doesn’t require a string to be written in UTF-8,
but it strongly encourages it. We’ll see how to work with UTF-8 strings in
upcoming chapters.
